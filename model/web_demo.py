import streamlit as st
import torch
import torch.nn as nn
import jieba
import pandas as pd
import os
import numpy as np
from collections import Counter
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go

# ==========================================
# 1. é…ç½®ä¸æ¨¡å‹å®šä¹‰
# ==========================================
MAX_LEN = 50
EMBEDDING_DIM = 100
HIDDEN_DIM = 128

class SentimentLSTM(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim=2):
        super(SentimentLSTM, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)
        
    def forward(self, text):
        embedded = self.embedding(text) 
        output, (hidden, cell) = self.lstm(embedded)
        final_hidden = hidden[-1] 
        return self.fc(final_hidden)

@st.cache_resource
def load_resources():
    current_dir = os.path.dirname(os.path.abspath(__file__))
    data_path = os.path.join(current_dir, '..', 'data', 'ChnSentiCorp_htl_all.csv')
    model_path = os.path.join(current_dir, 'sentiment_model.pth')

    if not os.path.exists(model_path): 
        # ä¸ºäº†æ¼”ç¤ºç³»ç»Ÿå®¹é”™æ€§ï¼Œè‹¥æ— æ¨¡å‹åˆ™è¿”å›ç©ºæ¨¡å‹å ä½ç¬¦
        return None, {"<PAD>": 0, "<UNK>": 1}, data_path, "æœªæ£€æµ‹åˆ°æ¨¡å‹æ–‡ä»¶ï¼Œç³»ç»Ÿå°†ä»¥æ¼”ç¤ºæ¨¡å¼è¿è¡Œã€‚"
    
    try:
        if os.path.exists(data_path):
            try:
                df = pd.read_csv(data_path)
            except:
                df = pd.read_csv(data_path, encoding='gbk')
            df = df.dropna(subset=['review'])
            all_words = [word for text in df['review'].astype(str) for word in jieba.lcut(text)]
        else:
            all_words = ["å¥½", "å·®", "æœåŠ¡", "ç¯å¢ƒ", "å¹²å‡€"] 
            
        vocab = {"<PAD>": 0, "<UNK>": 1}
        for word, _ in Counter(all_words).most_common(5000):
            vocab[word] = len(vocab)
    except Exception as e:
        return None, None, None, f"æ•°æ®åŠ è½½å¤±è´¥: {str(e)}"

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = SentimentLSTM(len(vocab), EMBEDDING_DIM, HIDDEN_DIM)
    try:
        model.load_state_dict(torch.load(model_path, map_location=device))
        model.eval()
        status = "SUCCESS"
    except Exception as e:
        model = None
        status = f"æ¨¡å‹æƒé‡åŠ è½½å¤±è´¥: {str(e)}"
    
    return model, vocab, data_path, status

# ==========================================
# 2. æ ¸å¿ƒåˆ†æä¸å¯è§†åŒ–å·¥å…·å‡½æ•°
# ==========================================
def analyze_aspect(text):
    """æå–è¯„è®ºä¸­æ¶‰åŠçš„å¤šä¸ªç»´åº¦ï¼Œè¿”å›åˆ—è¡¨"""
    aspects = {
        "å«ç”Ÿä¸è®¾æ–½": ["è„", "ä¹±", "è‡­", "å‘³é“", "æ—§", "ç ´", "å", "å¹²å‡€", "æ•´æ´", "é©¬æ¡¶", "åºŠ", "è®¾æ–½", "ç¡¬ä»¶"],
        "æœåŠ¡ä½“éªŒ": ["æœåŠ¡", "å‰å°", "æ€åº¦", "çƒ­æƒ…", "å†·æ·¡", "æ•ˆç‡", "æ…¢", "ä¿æ´", "ä¿å®‰"],
        "ä½ç½®ä¸äº¤é€š": ["ä½ç½®", "äº¤é€š", "åœ°é“", "å…¬äº¤", "ååƒ»", "æ–¹ä¾¿", "åµ", "éš”éŸ³", "å‘¨è¾¹", "å•†åœº"],
        "ä»·æ ¼ä¸æ€§ä»·æ¯”": ["ä»·æ ¼", "è´µ", "ä¾¿å®œ", "æ€§ä»·æ¯”", "åˆ’ç®—", "å€¼", "æ”¶è´¹"]
    }
    text = str(text)
    detected = [k for k, v in aspects.items() if any(kw in text for kw in v)]
    return detected if detected else ["å…¶ä»–/æœªæåŠ"]

def generate_wordcloud(text_list, custom_stop_words=""):
    """ç”Ÿæˆè¯äº‘å›¾å¹¶è¿”å› matplotlib figure"""
    if not text_list: return None
    base_stop_words = {"çš„", "æ˜¯", "äº†", "åœ¨", "æˆ‘", "æˆ‘ä»¬", "ä½ ", "æœ‰", "å’Œ", "å°±", "ä¸", "äºº", "éƒ½", "ä¸€ä¸ª", "ä¸Š", "ä¹Ÿ", "å¾ˆ", "åˆ°", "è¯´", "å»", "ä¼š", "ç€", "æ²¡æœ‰", "ä½†æ˜¯", "å› ä¸º", "è¿˜æ˜¯", "è¿™", "é‚£", "ä¸ª", "ä½", "å¯¹", "è®©", "ç»™", "æŠŠ", "è¢«", "è·Ÿ", "ä¸", "ä¸º", "ç­‰", "æ„Ÿè§‰", "è§‰å¾—"}
    
    # èåˆè‡ªå®šä¹‰åœç”¨è¯
    if custom_stop_words:
        base_stop_words.update(set(custom_stop_words.replace("ï¼Œ", ",").split(",")))
    
    full_text = " ".join([str(t) for t in text_list])
    words = jieba.lcut(full_text)
    clean_words = [w for w in words if w not in base_stop_words and len(w) > 1]
    if not clean_words: return None
    cut_text = " ".join(clean_words)
    
    font_path = "simhei.ttf" 
    if os.path.exists("C:/Windows/Fonts/simhei.ttf"): font_path = "C:/Windows/Fonts/simhei.ttf"
    elif os.path.exists("C:/Windows/Fonts/msyh.ttc"): font_path = "C:/Windows/Fonts/msyh.ttc"
    
    wc = WordCloud(font_path=font_path, background_color='white', width=800, height=400, max_words=100, font_step=2, collocations=False).generate(cut_text)
    fig, ax = plt.subplots(figsize=(8, 4))
    ax.imshow(wc, interpolation='bilinear')
    ax.axis('off') 
    plt.subplots_adjust(top=1, bottom=0, right=1, left=0, hspace=0, wspace=0)
    return fig

def predict_sentiment(texts, model, vocab):
    """æ‰¹é‡é¢„æµ‹æ ¸å¿ƒé€»è¾‘"""
    if not model:
        # å…œåº•é€»è¾‘ï¼šæ— æ¨¡å‹æ—¶éšæœºç”Ÿæˆç»“æœä¾›å‰ç«¯å¤§å±å±•ç¤ºæµ‹è¯•
        return ["å¥½è¯„" if np.random.rand() > 0.4 else "å·®è¯„" for _ in texts]
    
    input_ids = []
    for text in texts:
        words = jieba.lcut(str(text))
        ids = [vocab.get(w, 1) for w in words]
        ids = ids[:MAX_LEN] if len(ids) > MAX_LEN else ids + [0]*(MAX_LEN-len(ids))
        input_ids.append(ids)
    
    tensor_input = torch.tensor(input_ids, dtype=torch.long)
    device = next(model.parameters()).device
    tensor_input = tensor_input.to(device)
    
    with torch.no_grad():
        preds = torch.argmax(model(tensor_input), dim=1).cpu().tolist()
    return ["å¥½è¯„" if p == 1 else "å·®è¯„" for p in preds]

# ==========================================
# 3. é¡µé¢æ¸²æŸ“æ¨¡å— (å¤§å±å¯è§†åŒ–)
# ==========================================
def render_dashboard(df):
    """æ¸²æŸ“æ•°æ®å¯è§†åŒ–å¤§å±"""
    st.markdown("### ğŸ“ˆ èˆ†æƒ…æ•°æ®ç›‘æ§å¤§å±")
    
    # --- æ ¸å¿ƒæŒ‡æ ‡åŒº ---
    total = len(df)
    pos_count = len(df[df['é¢„æµ‹ç»“æœ'] == 'å¥½è¯„'])
    neg_count = len(df[df['é¢„æµ‹ç»“æœ'] == 'å·®è¯„'])
    pos_rate = pos_count / total if total > 0 else 0
    
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("æ€»å¤„ç†è¯„è®ºæ•°", f"{total:,} æ¡", "+æ›´æ–°")
    with col2:
        st.metric("æ€»ä½“å¥½è¯„æ•°", f"{pos_count:,} æ¡", f"{pos_rate:.1%} å æ¯”")
    with col3:
        st.metric("æ€»ä½“å·®è¯„æ•°", f"{neg_count:,} æ¡", f"{1-pos_rate:.1%} å æ¯”", delta_color="inverse")
    with col4:
        st.metric("åˆ†ææ¨¡å‹çŠ¶æ€", "åœ¨çº¿ (LSTM)", "æ­£å¸¸è¿è¡Œ")
    
    st.markdown("---")
    
    # --- å›¾è¡¨åŒº ---
    c1, c2 = st.columns([1, 1])
    
    with c1:
        # 1. æƒ…æ„Ÿåˆ†å¸ƒç¯å½¢å›¾
        fig_pie = px.pie(
            names=['å¥½è¯„', 'å·®è¯„'], 
            values=[pos_count, neg_count], 
            hole=0.4,
            title="æƒ…æ„Ÿææ€§æ€»ä½“åˆ†å¸ƒ",
            color_discrete_sequence=['#2ecc71', '#e74c3c']
        )
        fig_pie.update_traces(textposition='inside', textinfo='percent+label')
        st.plotly_chart(fig_pie, use_container_width=True)

    with c2:
        # 2. è¯„è®ºç»´åº¦å…³æ³¨åº¦æŸ±çŠ¶å›¾
        # å°†åˆ—è¡¨å±•å¼€ç»Ÿè®¡
        all_aspects = [aspect for sublist in df['ç»´åº¦åˆ—è¡¨'] for aspect in sublist]
        aspect_counts = pd.Series(all_aspects).value_counts().reset_index()
        aspect_counts.columns = ['ç»´åº¦', 'æåŠé¢‘æ¬¡']
        
        fig_bar = px.bar(
            aspect_counts, 
            x='æåŠé¢‘æ¬¡', 
            y='ç»´åº¦', 
            orientation='h',
            title="æ¶ˆè´¹è€…æ ¸å¿ƒå…³æ³¨ç»´åº¦Topæ’è¡Œ",
            color='æåŠé¢‘æ¬¡',
            color_continuous_scale='Blues'
        )
        fig_bar.update_layout(yaxis={'categoryorder':'total ascending'})
        st.plotly_chart(fig_bar, use_container_width=True)

    # 3. ç»´åº¦ä¸æƒ…æ„Ÿçš„äº¤å‰åˆ†æ (å †å æŸ±çŠ¶å›¾)
    st.markdown("#### ğŸ¯ æ ¸å¿ƒç»´åº¦æƒ…æ„Ÿäº¤å‰åˆ†æ")
    # æ„å»ºäº¤å‰è¡¨
    aspect_sentiment_data = []
    for _, row in df.iterrows():
        for aspect in row['ç»´åº¦åˆ—è¡¨']:
            aspect_sentiment_data.append({'ç»´åº¦': aspect, 'æƒ…æ„Ÿ': row['é¢„æµ‹ç»“æœ']})
    
    cross_df = pd.DataFrame(aspect_sentiment_data)
    if not cross_df.empty:
        cross_table = pd.crosstab(cross_df['ç»´åº¦'], cross_df['æƒ…æ„Ÿ']).reset_index()
        
        # ç¡®ä¿åˆ—å­˜åœ¨
        for col in ['å¥½è¯„', 'å·®è¯„']:
            if col not in cross_table.columns: cross_table[col] = 0
            
        fig_stack = go.Figure(data=[
            go.Bar(name='å·®è¯„', x=cross_table['ç»´åº¦'], y=cross_table['å·®è¯„'], marker_color='#e74c3c'),
            go.Bar(name='å¥½è¯„', x=cross_table['ç»´åº¦'], y=cross_table['å¥½è¯„'], marker_color='#2ecc71')
        ])
        fig_stack.update_layout(barmode='stack', title="å„ç»´åº¦æƒ…æ„Ÿå€¾å‘æ„æˆæ¯”", xaxis_title="è¯„ä»·ç»´åº¦", yaxis_title="è¯„è®ºæ•°é‡")
        st.plotly_chart(fig_stack, use_container_width=True)


# ==========================================
# 4. ä¸»ç¨‹åºå…¥å£ä¸è·¯ç”±
# ==========================================
def main():
    st.set_page_config(page_title="æ™ºèƒ½æ–‡æœ¬æŒ–æ˜ä¸æƒ…æ„Ÿåˆ†æç³»ç»Ÿ", page_icon="ğŸ¢", layout="wide", initial_sidebar_state="expanded")
    
    # åˆå§‹åŒ–çŠ¶æ€
    if 'global_stop_words' not in st.session_state:
        st.session_state['global_stop_words'] = "é…’åº—,å®¾é¦†,å…¥ä½"

    with st.spinner('ç³»ç»Ÿå†…æ ¸åˆå§‹åŒ–ä¸­...'):
        model, vocab, default_data_path, status = load_resources()
        
    # ä¾§è¾¹æ å¯¼èˆª
    st.sidebar.title("ğŸ¢ æ–‡æœ¬æŒ–æ˜ç³»ç»Ÿ")
    st.sidebar.markdown("---")
    app_mode = st.sidebar.radio("ç³»ç»ŸåŠŸèƒ½å¯¼èˆª", [
        "ğŸ“Š ç›‘æ§å¤§å± (Dashboard)", 
        "ğŸ“ å•æ¡è¯Šæ–­ (Single Test)", 
        "ğŸ“‚ æ‰¹é‡æŒ–æ˜ (Batch Mining)",
        "âš™ï¸ ç³»ç»Ÿè®¾ç½® (Settings)"
    ])
    
    st.sidebar.markdown("---")
    st.sidebar.caption("Model Status: " + ("âœ… Online" if model else "âš ï¸ Not Found/Demo Mode"))

    # === åŠŸèƒ½è·¯ç”± ===

    if app_mode == "âš™ï¸ ç³»ç»Ÿè®¾ç½® (Settings)":
        st.title("âš™ï¸ ç³»ç»Ÿå‚æ•°é…ç½®")
        st.markdown("åœ¨æ­¤é…ç½®æ•°æ®å¤„ç†åŠæ¨¡å‹æ¨æ–­çš„ç›¸å…³å‚æ•°ã€‚")
        
        with st.form("config_form"):
            st.subheader("æ–‡æœ¬é¢„å¤„ç†é…ç½®")
            stop_words_input = st.text_area("è‡ªå®šä¹‰åœç”¨è¯ (ä»¥è‹±æ–‡é€—å·åˆ†éš”):", value=st.session_state['global_stop_words'])
            
            st.subheader("æ¨¡å‹æ¨æ–­å‚æ•°")
            conf_threshold = st.slider("åˆ¤å®šç½®ä¿¡åº¦é˜ˆå€¼ (ä½äºæ­¤å€¼æ ‡è®°ä¸º'ç–‘ä¼¼'):", 0.5, 0.99, 0.8)
            
            submitted = st.form_submit_button("ä¿å­˜ç³»ç»Ÿé…ç½®")
            if submitted:
                st.session_state['global_stop_words'] = stop_words_input
                st.success("ç³»ç»Ÿé…ç½®å·²æ›´æ–°å¹¶ç”Ÿæ•ˆã€‚")

    elif app_mode == "ğŸ“ å•æ¡è¯Šæ–­ (Single Test)":
        st.title("ğŸ“ å•æ–‡æœ¬æƒ…æ„Ÿè¯Šæ–­")
        st.markdown("è¾“å…¥å•æ¡å®¢æˆ·è¯„è®ºï¼Œç³»ç»Ÿå°†å®æ—¶è¾“å‡ºæƒ…æ„Ÿææ€§ã€ç½®ä¿¡åº¦åŠæå–çš„æ ¸å¿ƒä¸šåŠ¡ç»´åº¦ã€‚")
        
        with st.form("single_analysis_form"):
            user_input = st.text_area("ğŸ“„ å¾…æµ‹æ–‡æœ¬è¾“å…¥åŒº:", height=150, placeholder="è¯·è¾“å…¥ä¸€æ®µæ¶‰åŠäº§å“æˆ–æœåŠ¡çš„è¯„è®ºæ–‡æœ¬...")
            submit_btn = st.form_submit_button("è¿è¡Œåˆ†æé¢„æµ‹")
            
        if submit_btn and user_input.strip():
            with st.spinner("ç¥ç»ç½‘ç»œæ¨æ–­ä¸­..."):
                aspects = analyze_aspect(user_input)
                
                # æ¨¡å‹æ¨æ–­é€»è¾‘
                if model:
                    words = jieba.lcut(user_input)
                    ids = [vocab.get(w, 1) for w in words]
                    ids = ids[:MAX_LEN] if len(ids) > MAX_LEN else ids + [0]*(MAX_LEN-len(ids))
                    tensor_input = torch.tensor([ids], dtype=torch.long)
                    with torch.no_grad():
                        prob = torch.nn.functional.softmax(model(tensor_input), dim=1)
                        pred_class = torch.argmax(prob).item()
                        conf = prob[0][pred_class].item()
                    res_label = "å¥½è¯„" if pred_class == 1 else "å·®è¯„"
                else:
                    # å…œåº•æ¼”ç¤º
                    res_label = "å¥½è¯„" if "å¥½" in user_input else "å·®è¯„"
                    conf = 0.95
                
            st.markdown("### è¯Šæ–­æŠ¥å‘Š")
            col_res1, col_res2, col_res3 = st.columns(3)
            with col_res1:
                if res_label == "å¥½è¯„":
                    st.success(f"**åˆ¤å®šææ€§ï¼šæ­£å‘ (Positive)**")
                else:
                    st.error(f"**åˆ¤å®šææ€§ï¼šè´Ÿå‘ (Negative)**")
            with col_res2:
                st.info(f"**æ¨¡å‹ç½®ä¿¡åº¦ï¼š{conf:.2%}**")
            with col_res3:
                st.warning(f"**æ¶‰åŠç»´åº¦ï¼š{', '.join(aspects)}**")

    elif app_mode == "ğŸ“‚ æ‰¹é‡æŒ–æ˜ (Batch Mining)":
        st.title("ğŸ“‚ æ•°æ®æ‰¹é‡å¯¼å…¥ä¸æŒ–æ˜")
        st.markdown("æ”¯æŒå¯¼å…¥ CSV æ–‡ä»¶ï¼Œæ‰§è¡Œå¤§è§„æ¨¡æ•°æ®çš„æƒ…æ„Ÿè®¡ç®—ä¸ç»´åº¦æ‰“æ ‡ã€‚")
        
        data_source = st.radio("é€‰æ‹©æ•°æ®æºæ–¹å¼:", ["ğŸ“‚ æœ¬åœ°ä¸Šä¼ æ–‡ä»¶", "ğŸ åŠ è½½ç³»ç»Ÿæ¼”ç¤ºæ•°æ®é›†"], horizontal=True)
        
        df = None
        if data_source == "ğŸ“‚ æœ¬åœ°ä¸Šä¼ æ–‡ä»¶":
            uploaded_file = st.file_uploader("è¯·é€‰æ‹© CSV æ ¼å¼æ•°æ®æ–‡ä»¶", type=["csv"])
            if uploaded_file:
                try:
                    df = pd.read_csv(uploaded_file)
                except UnicodeDecodeError:
                    uploaded_file.seek(0)
                    df = pd.read_csv(uploaded_file, encoding='gbk')
        else:
            if st.button("ä¸€é”®åŠ è½½æµ‹è¯•æ ·æœ¬"):
                if default_data_path and os.path.exists(default_data_path):
                    df = pd.read_csv(default_data_path).sample(500)
                else:
                    st.error("æœªæ‰¾åˆ°æ¼”ç¤ºæ•°æ®é›†ã€‚")
                    
        if df is not None:
            # è‡ªåŠ¨è¯†åˆ«æ–‡æœ¬åˆ—
            cols = df.columns.tolist()
            keywords = ['review', 'è¯„è®º', 'content', 'text', 'å†…å®¹']
            text_col = cols[0] 
            for col in cols:
                if any(k in col.lower() for k in keywords):
                    text_col = col
                    break
            
            st.info(f"ğŸ’¡ ç³»ç»Ÿå·²è‡ªåŠ¨å°†å­—æ®µ `[{text_col}]` è¯†åˆ«ä¸ºåˆ†æå¯¹è±¡ã€‚")
            st.dataframe(df.head(5), use_container_width=True)
            
            if st.button("ğŸš€ å¯åŠ¨å…¨é‡æ·±åº¦åˆ†æ", type="primary"):
                progress_bar = st.progress(0)
                with st.spinner("æ‰§è¡Œè‡ªç„¶è¯­è¨€å¤„ç†æµæ°´çº¿..."):
                    texts = df[text_col].astype(str).tolist()
                    
                    # æ‰¹é‡é¢„æµ‹
                    df['é¢„æµ‹ç»“æœ'] = predict_sentiment(texts, model, vocab)
                    progress_bar.progress(50)
                    
                    # ç»´åº¦æ‰“æ ‡
                    df['ç»´åº¦åˆ—è¡¨'] = df[text_col].apply(analyze_aspect)
                    df['æ¶‰åŠç»´åº¦'] = df['ç»´åº¦åˆ—è¡¨'].apply(lambda x: ", ".join(x))
                    progress_bar.progress(100)
                    
                    st.session_state['master_df'] = df
                    st.session_state['text_col'] = text_col
                    st.success("æ‰¹é‡åˆ†æä»»åŠ¡å®Œæˆï¼è¯·å‰å¾€ã€Œç›‘æ§å¤§å±ã€æŸ¥çœ‹å¯è§†åŒ–ç»“æœï¼Œæˆ–åœ¨æ­¤å¤„ä¸‹è½½åŸå§‹æ•°æ®ã€‚")
            
        if 'master_df' in st.session_state:
            st.markdown("### æ•°æ®å¯¼å‡ºä¸æ£€ç´¢")
            res_df = st.session_state['master_df']
            
            search_term = st.text_input("åœ¨ç»“æœä¸­å…¨å±€æ£€ç´¢å…³é”®å­—:")
            if search_term:
                mask = res_df.astype(str).apply(lambda x: x.str.contains(search_term, case=False)).any(axis=1)
                display_df = res_df[mask]
            else:
                display_df = res_df
                
            st.dataframe(display_df.drop(columns=['ç»´åº¦åˆ—è¡¨'], errors='ignore'), height=300)
            
            csv_data = display_df.drop(columns=['ç»´åº¦åˆ—è¡¨'], errors='ignore').to_csv(index=False).encode('utf-8-sig')
            st.download_button("ğŸ“¥ å¯¼å‡ºåˆ†æç»“æœåŒ… (CSV)", csv_data, 'system_analysis_output.csv', 'text/csv')

    elif app_mode == "ğŸ“Š ç›‘æ§å¤§å± (Dashboard)":
        st.title("ğŸ“Š å…¨å±€æ•°æ®ç›‘æ§ä¸å¯è§†åŒ–")
        
        if 'master_df' not in st.session_state:
            st.warning("å½“å‰ç³»ç»Ÿæš‚æ— å¤„ç†å®Œæˆçš„æ•°æ®ã€‚è¯·å…ˆè¿›å…¥ã€ŒğŸ“‚ æ‰¹é‡æŒ–æ˜ã€æ¨¡å—å¤„ç†æ•°æ®ã€‚")
        else:
            df = st.session_state['master_df']
            render_dashboard(df)
            
            st.markdown("### â˜ï¸ é«˜é¢‘è¯äº‘ç‰¹å¾æå–")
            c1, c2 = st.columns(2)
            with c1:
                st.markdown("**æ­£å‘æƒ…æ„Ÿè¯äº‘ (Positive)**")
                pos_texts = df[df['é¢„æµ‹ç»“æœ'] == 'å¥½è¯„'][st.session_state.get('text_col', 'review')].tolist()
                fig_pos = generate_wordcloud(pos_texts, st.session_state['global_stop_words'])
                if fig_pos: st.pyplot(fig_pos)
            with c2:
                st.markdown("**è´Ÿå‘æƒ…æ„Ÿè¯äº‘ (Negative)**")
                neg_texts = df[df['é¢„æµ‹ç»“æœ'] == 'å·®è¯„'][st.session_state.get('text_col', 'review')].tolist()
                fig_neg = generate_wordcloud(neg_texts, st.session_state['global_stop_words'])
                if fig_neg: st.pyplot(fig_neg)

if __name__ == "__main__":
    main()