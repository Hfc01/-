import streamlit as st
import torch
import torch.nn as nn
import jieba
import pandas as pd
import os
import numpy as np
from collections import Counter
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# ==========================================
# 1. é…ç½®ä¸æ¨¡å‹å®šä¹‰
# ==========================================
MAX_LEN = 50
EMBEDDING_DIM = 100
HIDDEN_DIM = 128

class SentimentLSTM(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim=2):
        super(SentimentLSTM, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)
        
    def forward(self, text):
        embedded = self.embedding(text) 
        output, (hidden, cell) = self.lstm(embedded)
        final_hidden = hidden[-1] 
        return self.fc(final_hidden)

@st.cache_resource
def load_resources():
    current_dir = os.path.dirname(os.path.abspath(__file__))
    data_path = os.path.join(current_dir, '..', 'data', 'ChnSentiCorp_htl_all.csv')
    model_path = os.path.join(current_dir, 'sentiment_model.pth')

    if not os.path.exists(model_path): return None, None, None, "æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ï¼"
    
    try:
        if os.path.exists(data_path):
            try:
                df = pd.read_csv(data_path)
            except:
                df = pd.read_csv(data_path, encoding='gbk')
            df = df.dropna(subset=['review'])
            all_words = [word for text in df['review'].astype(str) for word in jieba.lcut(text)]
        else:
            all_words = ["å¥½", "å·®"] 
            
        vocab = {"<PAD>": 0, "<UNK>": 1}
        for word, _ in Counter(all_words).most_common(5000):
            vocab[word] = len(vocab)
    except:
        return None, None, None, "æ•°æ®åŠ è½½å¤±è´¥"

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = SentimentLSTM(len(vocab), EMBEDDING_DIM, HIDDEN_DIM)
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()
    
    return model, vocab, data_path, "SUCCESS"

# ==========================================
# 2. å·¥å…·å‡½æ•°
# ==========================================
def clear_cache():
    if 'result_df' in st.session_state: del st.session_state['result_df']
    if 'analyzed' in st.session_state: del st.session_state['analyzed']
    if 'csv_data' in st.session_state: del st.session_state['csv_data']

def generate_wordcloud(text_list, title):
    if not text_list: return None
    stop_words = {"çš„", "æ˜¯", "äº†", "åœ¨", "æˆ‘", "æˆ‘ä»¬", "ä½ ", "æœ‰", "å’Œ", "å°±", "ä¸", "äºº", "éƒ½", "ä¸€ä¸ª", "ä¸Š", "ä¹Ÿ", "å¾ˆ", "åˆ°", "è¯´", "å»", "ä¼š", "ç€", "æ²¡æœ‰", "ä½†æ˜¯", "å› ä¸º", "è¿˜æ˜¯", "è¿™", "é‚£", "ä¸ª", "ä½", "å¯¹", "è®©", "ç»™", "æŠŠ", "è¢«", "è·Ÿ", "ä¸", "ä¸º", "ç­‰", "é…’åº—", "å®¾é¦†", "æ„Ÿè§‰", "è§‰å¾—"}
    
    full_text = " ".join([str(t) for t in text_list])
    words = jieba.lcut(full_text)
    clean_words = [w for w in words if w not in stop_words and len(w) > 1]
    if not clean_words: return None
    cut_text = " ".join(clean_words)
    
    font_path = "simhei.ttf" 
    if os.path.exists("C:/Windows/Fonts/simhei.ttf"): font_path = "C:/Windows/Fonts/simhei.ttf"
    elif os.path.exists("C:/Windows/Fonts/msyh.ttc"): font_path = "C:/Windows/Fonts/msyh.ttc"
    
    wc = WordCloud(font_path=font_path, background_color='white', width=1000, height=800, max_words=100, font_step=2, collocations=False).generate(cut_text)
    fig, ax = plt.subplots(figsize=(8, 6))
    ax.imshow(wc, interpolation='bilinear')
    ax.axis('off') 
    plt.subplots_adjust(top=1, bottom=0, right=1, left=0, hspace=0, wspace=0)
    return fig

def analyze_aspect(text):
    aspects = {
        "å«ç”Ÿ/è®¾æ–½": ["è„", "ä¹±", "è‡­", "å‘³é“", "æ—§", "ç ´", "å", "å¹²å‡€", "æ•´æ´", "é©¬æ¡¶", "åºŠ"],
        "æœåŠ¡æ€åº¦": ["æœåŠ¡", "å‰å°", "æ€åº¦", "çƒ­æƒ…", "å†·æ·¡", "æ•ˆç‡", "æ…¢"],
        "ä½ç½®/äº¤é€š": ["ä½ç½®", "äº¤é€š", "åœ°é“", "å…¬äº¤", "ååƒ»", "æ–¹ä¾¿", "åµ", "éš”éŸ³"],
        "ä»·æ ¼/æ€§ä»·æ¯”": ["ä»·æ ¼", "è´µ", "ä¾¿å®œ", "æ€§ä»·æ¯”", "åˆ’ç®—", "å€¼"]
    }
    text = str(text)
    detected = [k for k, v in aspects.items() if any(kw in text for kw in v)]
    return ", ".join(detected) if detected else "å…¶ä»–/æœªæåŠ"

# ==========================================
# 3. ä¸»ç¨‹åºå…¥å£
# ==========================================
def main():
    st.set_page_config(page_title="ç”µå•†è¯„è®ºåˆ†æç³»ç»ŸPro", page_icon="ğŸ›ï¸", layout="wide")
    st.title("ğŸ›ï¸ ç”µå•†è¯„è®ºæƒ…æ„Ÿåˆ†æç³»ç»Ÿ")

    with st.spinner('æ­£åœ¨åˆå§‹åŒ– AI å¤§è„‘...'):
        model, vocab, default_data_path, status = load_resources()
    if status != "SUCCESS": st.error(status); return

    st.sidebar.header("ğŸ•¹ï¸ æ§åˆ¶å°")
    app_mode = st.sidebar.radio("é€‰æ‹©æ¨¡å¼", ["å•æ¡åˆ†æ", "æ‰¹é‡åˆ†æ"], on_change=clear_cache)

    # === å•æ¡åˆ†æ ===
    if app_mode == "å•æ¡åˆ†æ":
        st.subheader("ğŸ“ å•æ¡è¯„è®ºé¢„æµ‹")
        col1, col2 = st.columns([3, 2])
        with col1:
            user_input = st.text_area("è¾“å…¥è¯„è®º:", height=150)
            if st.button("åˆ†æ", type="primary"):
                if user_input.strip():
                    words = jieba.lcut(user_input)
                    ids = [vocab.get(w, 1) for w in words]
                    ids = ids[:MAX_LEN] if len(ids) > MAX_LEN else ids + [0]*(MAX_LEN-len(ids))
                    tensor_input = torch.tensor([ids], dtype=torch.long)
                    with torch.no_grad():
                        prob = torch.nn.functional.softmax(model(tensor_input), dim=1)
                        pred_class = torch.argmax(prob).item()
                        conf = prob[0][pred_class].item()
                    aspect_info = analyze_aspect(user_input)
                    with col2:
                        st.markdown("### ç»“æœ")
                        if pred_class == 1: st.success(f"**ğŸ˜Š å¥½è¯„** ({conf:.2%})")
                        else: st.error(f"**ğŸ˜¡ å·®è¯„** ({conf:.2%})")
                        st.info(f"ç»´åº¦ï¼š{aspect_info}")

    # === æ‰¹é‡åˆ†æ (æç®€ç‰ˆ) ===
    elif app_mode == "æ‰¹é‡åˆ†æ":
        st.subheader("ğŸ“Š æ‰¹é‡æ•°æ®å¤„ç†")
        
        data_source = st.radio("æ•°æ®æ¥æº:", ["ğŸ“‚ ä¸Šä¼  CSV", "ğŸ æ¼”ç¤ºæ•°æ®"], horizontal=True, on_change=clear_cache)
        
        df = None
        
        if data_source == "ğŸ“‚ ä¸Šä¼  CSV":
            uploaded_file = st.file_uploader("ä¸Šä¼ æ–‡ä»¶", type=["csv"])
            if uploaded_file:
                try:
                    df = pd.read_csv(uploaded_file)
                except UnicodeDecodeError:
                    uploaded_file.seek(0)
                    df = pd.read_csv(uploaded_file, encoding='gbk')
        else:
            if st.button("åŠ è½½æ¼”ç¤ºæ•°æ®"):
                if default_data_path and os.path.exists(default_data_path):
                    try:
                        df = pd.read_csv(default_data_path).sample(1000)
                        st.session_state['temp_df'] = df 
                        clear_cache()
                    except: st.error("è¯»å–å¤±è´¥")
            if 'temp_df' in st.session_state and data_source == "ğŸ æ¼”ç¤ºæ•°æ®":
                df = st.session_state['temp_df']

        if df is not None:
            # âœ¨ æ ¸å¿ƒä¿®æ”¹ï¼šå®Œå…¨è‡ªåŠ¨åŒ–åˆ—åè¯†åˆ«ï¼Œä¸è®©ç”¨æˆ·é€‰ âœ¨
            cols = df.columns.tolist()
            # ä¼˜å…ˆçº§å…³é”®è¯åˆ—è¡¨
            keywords = ['review', 'è¯„è®º', 'content', 'text', 'å†…å®¹', 'category', 'feedback']
            text_col = cols[0] # é»˜è®¤ç¬¬ä¸€åˆ—ï¼Œé˜²å´©
            
            # æ™ºèƒ½åŒ¹é…
            for col in cols:
                if any(k in col.lower() for k in keywords):
                    text_col = col
                    break
            
            # ä»…ä»…å±•ç¤ºä¸€è¡Œå°å­—å‘ŠçŸ¥ç”¨æˆ·
            st.info(f"âœ… å·²è‡ªåŠ¨è¯†åˆ«æ–‡æœ¬åˆ—ï¼š**{text_col}**")
            
            if st.button("ğŸš€ å¼€å§‹åˆ†æ", type="primary"):
                with st.spinner("å¤„ç†ä¸­..."):
                    texts = df[text_col].astype(str).tolist()
                    input_ids = []
                    for text in texts:
                        words = jieba.lcut(text)
                        ids = [vocab.get(w, 1) for w in words]
                        ids = ids[:MAX_LEN] if len(ids) > MAX_LEN else ids + [0]*(MAX_LEN-len(ids))
                        input_ids.append(ids)
                    
                    tensor_input = torch.tensor(input_ids, dtype=torch.long)
                    with torch.no_grad():
                        preds = torch.argmax(model(tensor_input), dim=1).tolist()
                    
                    df['é¢„æµ‹ç»“æœ'] = ['å¥½è¯„' if p==1 else 'å·®è¯„' for p in preds]
                    df['æ¶‰åŠç»´åº¦'] = df[text_col].apply(analyze_aspect)
                    
                    csv_data = df.to_csv(index=False).encode('utf-8-sig')
                    st.session_state['result_df'] = df
                    st.session_state['csv_data'] = csv_data
                    st.session_state['analyzed'] = True
                    st.rerun() 

        # ç»“æœåŒº
        if st.session_state.get('analyzed') and 'result_df' in st.session_state:
            res_df = st.session_state['result_df']
            
            st.markdown("---")
            
            # ç­›é€‰
            col_filter1, col_filter2 = st.columns(2)
            with col_filter1:
                filter_sentiment = st.multiselect("æƒ…æ„Ÿç­›é€‰:", ["å¥½è¯„", "å·®è¯„"], default=["å¥½è¯„", "å·®è¯„"])
            with col_filter2:
                filter_keyword = st.text_input("å…³é”®è¯æœç´¢:")
            
            filtered_df = res_df.copy()
            # è¿™é‡Œçš„ text_col éœ€è¦é‡æ–°è·å–ä¸€ä¸‹ï¼Œæˆ–è€…ç®€å•ç²—æš´éå†æ‰€æœ‰åˆ—ï¼Œ
            # ä½†ä¸ºäº†ç®€å•ï¼Œæˆ‘ä»¬ç›´æ¥ç”¨ session_state é‡Œçš„æ•°æ®ï¼Œä¸ç”¨ç®¡åˆ—åäº†
            # ä¸ºäº†ç­›é€‰å…³é”®è¯ï¼Œæˆ‘ä»¬å‡è®¾åŒ…å«'review'æˆ–'è¯„è®º'çš„åˆ—ï¼Œæˆ–è€…ç›´æ¥æœå…¨è¡¨
            if filter_sentiment: filtered_df = filtered_df[filtered_df['é¢„æµ‹ç»“æœ'].isin(filter_sentiment)]
            if filter_keyword: 
                # ç®€å•ç²—æš´æœç´¢æ‰€æœ‰åˆ—ï¼Œçœå»éº»çƒ¦
                mask = filtered_df.astype(str).apply(lambda x: x.str.contains(filter_keyword, case=False)).any(axis=1)
                filtered_df = filtered_df[mask]

            kpi1, kpi2, kpi3 = st.columns(3)
            kpi1.metric("æ•°é‡", f"{len(filtered_df)}")
            kpi2.metric("å¥½è¯„", f"{len(filtered_df[filtered_df['é¢„æµ‹ç»“æœ']=='å¥½è¯„'])}")
            kpi3.metric("å·®è¯„", f"{len(filtered_df[filtered_df['é¢„æµ‹ç»“æœ']=='å·®è¯„'])}")
            
            c1, c2 = st.columns(2)
            with c1:
                if not filtered_df.empty: st.bar_chart(filtered_df['é¢„æµ‹ç»“æœ'].value_counts())
            with c2:
                # é‡æ–°å¯»æ‰¾æ–‡æœ¬åˆ—ç”¨äºç”»å›¾
                cols = filtered_df.columns.tolist()
                keywords = ['review', 'è¯„è®º', 'content', 'text', 'category']
                target_col = cols[0]
                for col in cols:
                    if any(k in col.lower() for k in keywords):
                        target_col = col
                        break
                if not filtered_df.empty:
                    fig = generate_wordcloud(filtered_df[target_col].tolist(), "")
                    if fig: st.pyplot(fig)
            
            if 'csv_data' in st.session_state:
                st.download_button("ğŸ“¥ ä¸‹è½½ç»“æœ (CSV)", st.session_state['csv_data'], 'result.csv', 'text/csv', type='primary')
            
            with st.expander("è¯¦ç»†æ•°æ®"):
                st.dataframe(filtered_df)

if __name__ == "__main__":
    main()