import streamlit as st
import torch
import torch.nn as nn
import jieba
import pandas as pd
import os
import numpy as np
from collections import Counter
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go
import sqlite3
import hashlib

# ==========================================
# 0. ç”¨æˆ·è®¤è¯ä¸æ•°æ®åº“æ¨¡å—
# ==========================================
def make_hashes(password):
    return hashlib.sha256(str.encode(password)).hexdigest()

def check_hashes(password, hashed_text):
    if make_hashes(password) == hashed_text:
        return True
    return False

def create_usertable():
    conn = sqlite3.connect('users.db')
    c = conn.cursor()
    c.execute('CREATE TABLE IF NOT EXISTS userstable(username TEXT, password TEXT)')
    conn.commit()
    conn.close()

def add_userdata(username, password):
    conn = sqlite3.connect('users.db')
    c = conn.cursor()
    c.execute('INSERT INTO userstable(username, password) VALUES (?,?)', (username, password))
    conn.commit()
    conn.close()

def login_user(username, password):
    conn = sqlite3.connect('users.db')
    c = conn.cursor()
    c.execute('SELECT * FROM userstable WHERE username =? AND password = ?', (username, password))
    data = c.fetchall()
    conn.close()
    return data

# ==========================================
# 1. é…ç½®ä¸æ¨¡å‹å®šä¹‰
# ==========================================
MAX_LEN = 50
EMBEDDING_DIM = 100
HIDDEN_DIM = 128

class SentimentLSTM(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim=2):
        super(SentimentLSTM, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)
        
    def forward(self, text):
        embedded = self.embedding(text) 
        output, (hidden, cell) = self.lstm(embedded)
        final_hidden = hidden[-1] 
        return self.fc(final_hidden)

@st.cache_resource
def load_resources():
    current_dir = os.path.dirname(os.path.abspath(__file__))
    data_path = os.path.join(current_dir, '..', 'data', 'ChnSentiCorp_htl_all.csv')
    model_path = os.path.join(current_dir, 'sentiment_model.pth')

    if not os.path.exists(model_path): 
        return None, {"<PAD>": 0, "<UNK>": 1}, data_path, "æœªæ£€æµ‹åˆ°æ¨¡å‹æ–‡ä»¶ï¼Œç³»ç»Ÿå°†ä»¥æ¼”ç¤ºæ¨¡å¼è¿è¡Œã€‚"
    
    try:
        if os.path.exists(data_path):
            try:
                df = pd.read_csv(data_path)
            except:
                df = pd.read_csv(data_path, encoding='gbk')
            df = df.dropna(subset=['review'])
            all_words = [word for text in df['review'].astype(str) for word in jieba.lcut(text)]
        else:
            all_words = ["å¥½", "å·®", "æœåŠ¡", "ç¯å¢ƒ", "å¹²å‡€"] 
            
        vocab = {"<PAD>": 0, "<UNK>": 1}
        for word, _ in Counter(all_words).most_common(5000):
            vocab[word] = len(vocab)
    except Exception as e:
        return None, None, None, f"æ•°æ®åŠ è½½å¤±è´¥: {str(e)}"

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = SentimentLSTM(len(vocab), EMBEDDING_DIM, HIDDEN_DIM)
    try:
        model.load_state_dict(torch.load(model_path, map_location=device))
        model.eval()
        status = "SUCCESS"
    except Exception as e:
        model = None
        status = f"æ¨¡å‹æƒé‡åŠ è½½å¤±è´¥: {str(e)}"
    
    return model, vocab, data_path, status

# ==========================================
# 2. æ ¸å¿ƒåˆ†æä¸å¯è§†åŒ–å·¥å…·å‡½æ•°
# ==========================================
def analyze_aspect(text):
    aspects = {
        "å«ç”Ÿä¸è®¾æ–½": ["è„", "ä¹±", "è‡­", "å‘³é“", "æ—§", "ç ´", "å", "å¹²å‡€", "æ•´æ´", "é©¬æ¡¶", "åºŠ", "è®¾æ–½", "ç¡¬ä»¶"],
        "æœåŠ¡ä½“éªŒ": ["æœåŠ¡", "å‰å°", "æ€åº¦", "çƒ­æƒ…", "å†·æ·¡", "æ•ˆç‡", "æ…¢", "ä¿æ´", "ä¿å®‰"],
        "ä½ç½®ä¸äº¤é€š": ["ä½ç½®", "äº¤é€š", "åœ°é“", "å…¬äº¤", "ååƒ»", "æ–¹ä¾¿", "åµ", "éš”éŸ³", "å‘¨è¾¹", "å•†åœº"],
        "ä»·æ ¼ä¸æ€§ä»·æ¯”": ["ä»·æ ¼", "è´µ", "ä¾¿å®œ", "æ€§ä»·æ¯”", "åˆ’ç®—", "å€¼", "æ”¶è´¹"]
    }
    text = str(text)
    detected = [k for k, v in aspects.items() if any(kw in text for kw in v)]
    return detected if detected else ["å…¶ä»–/æœªæåŠ"]

def generate_wordcloud(text_list, custom_stop_words=""):
    if not text_list: return None
    base_stop_words = {"çš„", "æ˜¯", "äº†", "åœ¨", "æˆ‘", "æˆ‘ä»¬", "ä½ ", "æœ‰", "å’Œ", "å°±", "ä¸", "äºº", "éƒ½", "ä¸€ä¸ª", "ä¸Š", "ä¹Ÿ", "å¾ˆ", "åˆ°", "è¯´", "å»", "ä¼š", "ç€", "æ²¡æœ‰", "ä½†æ˜¯", "å› ä¸º", "è¿˜æ˜¯", "è¿™", "é‚£", "ä¸ª", "ä½", "å¯¹", "è®©", "ç»™", "æŠŠ", "è¢«", "è·Ÿ", "ä¸", "ä¸º", "ç­‰", "æ„Ÿè§‰", "è§‰å¾—"}
    
    if custom_stop_words:
        base_stop_words.update(set(custom_stop_words.replace("ï¼Œ", ",").split(",")))
    
    full_text = " ".join([str(t) for t in text_list])
    words = jieba.lcut(full_text)
    clean_words = [w for w in words if w not in base_stop_words and len(w) > 1]
    if not clean_words: return None
    cut_text = " ".join(clean_words)
    
    font_path = "simhei.ttf" 
    if os.path.exists("C:/Windows/Fonts/simhei.ttf"): font_path = "C:/Windows/Fonts/simhei.ttf"
    elif os.path.exists("C:/Windows/Fonts/msyh.ttc"): font_path = "C:/Windows/Fonts/msyh.ttc"
    
    wc = WordCloud(font_path=font_path, background_color='white', width=800, height=400, max_words=100, font_step=2, collocations=False).generate(cut_text)
    fig, ax = plt.subplots(figsize=(8, 4))
    ax.imshow(wc, interpolation='bilinear')
    ax.axis('off') 
    plt.subplots_adjust(top=1, bottom=0, right=1, left=0, hspace=0, wspace=0)
    return fig

def predict_sentiment(texts, model, vocab):
    if not model:
        return ["å¥½è¯„" if np.random.rand() > 0.4 else "å·®è¯„" for _ in texts]
    
    input_ids = []
    for text in texts:
        words = jieba.lcut(str(text))
        ids = [vocab.get(w, 1) for w in words]
        ids = ids[:MAX_LEN] if len(ids) > MAX_LEN else ids + [0]*(MAX_LEN-len(ids))
        input_ids.append(ids)
    
    tensor_input = torch.tensor(input_ids, dtype=torch.long)
    device = next(model.parameters()).device
    tensor_input = tensor_input.to(device)
    
    with torch.no_grad():
        preds = torch.argmax(model(tensor_input), dim=1).cpu().tolist()
    return ["å¥½è¯„" if p == 1 else "å·®è¯„" for p in preds]

# ==========================================
# 3. é¡µé¢æ¸²æŸ“æ¨¡å—
# ==========================================
def render_dashboard(df):
    st.markdown("### ğŸ“ˆ èˆ†æƒ…æ•°æ®ç›‘æ§å¤§å±")
    
    total = len(df)
    pos_count = len(df[df['é¢„æµ‹ç»“æœ'] == 'å¥½è¯„'])
    neg_count = len(df[df['é¢„æµ‹ç»“æœ'] == 'å·®è¯„'])
    pos_rate = pos_count / total if total > 0 else 0
    
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("æ€»å¤„ç†è¯„è®ºæ•°", f"{total:,} æ¡", "+æ›´æ–°")
    with col2:
        st.metric("æ€»ä½“å¥½è¯„æ•°", f"{pos_count:,} æ¡", f"{pos_rate:.1%} å æ¯”")
    with col3:
        st.metric("æ€»ä½“å·®è¯„æ•°", f"{neg_count:,} æ¡", f"{1-pos_rate:.1%} å æ¯”", delta_color="inverse")
    with col4:
        st.metric("åˆ†ææ¨¡å‹çŠ¶æ€", "åœ¨çº¿ (LSTM)", "æ­£å¸¸è¿è¡Œ")
    
    st.markdown("---")
    
    c1, c2 = st.columns([1, 1])
    with c1:
        fig_pie = px.pie(
            names=['å¥½è¯„', 'å·®è¯„'], 
            values=[pos_count, neg_count], 
            hole=0.4,
            title="æƒ…æ„Ÿææ€§æ€»ä½“åˆ†å¸ƒ",
            color_discrete_sequence=['#2ecc71', '#e74c3c']
        )
        fig_pie.update_traces(textposition='inside', textinfo='percent+label')
        st.plotly_chart(fig_pie, use_container_width=True)

    with c2:
        all_aspects = [aspect for sublist in df['ç»´åº¦åˆ—è¡¨'] for aspect in sublist]
        aspect_counts = pd.Series(all_aspects).value_counts().reset_index()
        aspect_counts.columns = ['ç»´åº¦', 'æåŠé¢‘æ¬¡']
        
        fig_bar = px.bar(
            aspect_counts, 
            x='æåŠé¢‘æ¬¡', 
            y='ç»´åº¦', 
            orientation='h',
            title="æ¶ˆè´¹è€…æ ¸å¿ƒå…³æ³¨ç»´åº¦Topæ’è¡Œ",
            color='æåŠé¢‘æ¬¡',
            color_continuous_scale='Blues'
        )
        fig_bar.update_layout(yaxis={'categoryorder':'total ascending'})
        st.plotly_chart(fig_bar, use_container_width=True)

    st.markdown("#### ğŸ¯ æ ¸å¿ƒç»´åº¦æƒ…æ„Ÿäº¤å‰åˆ†æ")
    aspect_sentiment_data = []
    for _, row in df.iterrows():
        for aspect in row['ç»´åº¦åˆ—è¡¨']:
            aspect_sentiment_data.append({'ç»´åº¦': aspect, 'æƒ…æ„Ÿ': row['é¢„æµ‹ç»“æœ']})
    
    cross_df = pd.DataFrame(aspect_sentiment_data)
    if not cross_df.empty:
        cross_table = pd.crosstab(cross_df['ç»´åº¦'], cross_df['æƒ…æ„Ÿ']).reset_index()
        for col in ['å¥½è¯„', 'å·®è¯„']:
            if col not in cross_table.columns: cross_table[col] = 0
            
        fig_stack = go.Figure(data=[
            go.Bar(name='å·®è¯„', x=cross_table['ç»´åº¦'], y=cross_table['å·®è¯„'], marker_color='#e74c3c'),
            go.Bar(name='å¥½è¯„', x=cross_table['ç»´åº¦'], y=cross_table['å¥½è¯„'], marker_color='#2ecc71')
        ])
        fig_stack.update_layout(barmode='stack', title="å„ç»´åº¦æƒ…æ„Ÿå€¾å‘æ„æˆæ¯”", xaxis_title="è¯„ä»·ç»´åº¦", yaxis_title="è¯„è®ºæ•°é‡")
        st.plotly_chart(fig_stack, use_container_width=True)

# ==========================================
# 4. ç™»å½•é¡µé¢ä¸ä¸»ç¨‹åºå…¥å£
# ==========================================
def login_page():
    # ä¸ºäº†è®©ç™»å½•é¡µé¢æ›´ç®€æ´ï¼Œå±…ä¸­æ˜¾ç¤º
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        st.title("ğŸ” ç³»ç»Ÿç™»å½•")
        st.markdown("æ¬¢è¿è®¿é—®ç”µå•†è¯„è®ºæ™ºèƒ½åˆ†æå¹³å°ã€‚")
        
        create_usertable()
        
        tab1, tab2 = st.tabs(["ğŸ”‘ èº«ä»½éªŒè¯", "ğŸ“ æ³¨å†Œè´¦å·"])
        
        with tab1:
            username = st.text_input("ç”¨æˆ·å", key="login_user")
            password = st.text_input("å¯†ç ", type='password', key="login_pass")
            if st.button("ç™»å½•ç³»ç»Ÿ", type="primary", use_container_width=True):
                hashed_pswd = make_hashes(password)
                result = login_user(username, hashed_pswd)
                if result:
                    st.session_state['logged_in'] = True
                    st.session_state['current_user'] = username
                    st.success("éªŒè¯æˆåŠŸï¼Œæ­£åœ¨è¿›å…¥ç³»ç»Ÿ...")
                    st.rerun()
                else:
                    st.error("éªŒè¯å¤±è´¥ï¼šç”¨æˆ·åæˆ–å¯†ç é”™è¯¯ã€‚")

        with tab2:
            new_user = st.text_input("è®¾å®šç”¨æˆ·å", key="reg_user")
            new_password = st.text_input("è®¾å®šå¯†ç ", type='password', key="reg_pass")
            if st.button("æ³¨å†Œæ–°ç”¨æˆ·", use_container_width=True):
                if new_user and new_password:
                    add_userdata(new_user, make_hashes(new_password))
                    st.success("è´¦å·æ³¨å†ŒæˆåŠŸï¼Œè¯·åˆ‡æ¢è‡³ç™»å½•æ ‡ç­¾è¿›è¡Œæ“ä½œã€‚")
                else:
                    st.warning("æ³¨å†Œå¤±è´¥ï¼šéœ€å¡«å†™å®Œæ•´ä¿¡æ¯ã€‚")

def main():
    if 'global_stop_words' not in st.session_state:
        st.session_state['global_stop_words'] = "é…’åº—,å®¾é¦†,å…¥ä½"

    with st.spinner('ç³»ç»Ÿå†…æ ¸åˆå§‹åŒ–ä¸­...'):
        model, vocab, default_data_path, status = load_resources()
        
    st.sidebar.title("ğŸ¢ æ–‡æœ¬æŒ–æ˜ç³»ç»Ÿ")
    st.sidebar.markdown("---")
    app_mode = st.sidebar.radio("ç³»ç»ŸåŠŸèƒ½å¯¼èˆª", [
        "ğŸ“Š ç›‘æ§å¤§å± (Dashboard)", 
        "ğŸ“ å•æ¡è¯Šæ–­ (Single Test)", 
        "ğŸ“‚ æ‰¹é‡æŒ–æ˜ (Batch Mining)",
        "âš™ï¸ ç³»ç»Ÿè®¾ç½® (Settings)"
    ])
    
    st.sidebar.markdown("---")
    st.sidebar.caption("Model Status: " + ("âœ… Online" if model else "âš ï¸ Demo Mode"))
    st.sidebar.write(f"æ“ä½œå‘˜: **{st.session_state.get('current_user', 'Admin')}**")
    if st.sidebar.button("ğŸšª å®‰å…¨é€€å‡º"):
        st.session_state['logged_in'] = False
        st.rerun()

    if app_mode == "âš™ï¸ ç³»ç»Ÿè®¾ç½® (Settings)":
        st.title("âš™ï¸ ç³»ç»Ÿå‚æ•°é…ç½®")
        with st.form("config_form"):
            st.subheader("æ–‡æœ¬é¢„å¤„ç†é…ç½®")
            stop_words_input = st.text_area("è‡ªå®šä¹‰åœç”¨è¯ (ä»¥è‹±æ–‡é€—å·åˆ†éš”):", value=st.session_state['global_stop_words'])
            submitted = st.form_submit_button("ä¿å­˜ç³»ç»Ÿé…ç½®")
            if submitted:
                st.session_state['global_stop_words'] = stop_words_input
                st.success("ç³»ç»Ÿé…ç½®å·²æ›´æ–°å¹¶ç”Ÿæ•ˆã€‚")

    elif app_mode == "ğŸ“ å•æ¡è¯Šæ–­ (Single Test)":
        st.title("ğŸ“ å•æ–‡æœ¬æƒ…æ„Ÿè¯Šæ–­")
        with st.form("single_analysis_form"):
            user_input = st.text_area("ğŸ“„ å¾…æµ‹æ–‡æœ¬è¾“å…¥åŒº:", height=150)
            submit_btn = st.form_submit_button("è¿è¡Œåˆ†æé¢„æµ‹")
            
        if submit_btn and user_input.strip():
            with st.spinner("ç¥ç»ç½‘ç»œæ¨æ–­ä¸­..."):
                aspects = analyze_aspect(user_input)
                if model:
                    words = jieba.lcut(user_input)
                    ids = [vocab.get(w, 1) for w in words]
                    ids = ids[:MAX_LEN] if len(ids) > MAX_LEN else ids + [0]*(MAX_LEN-len(ids))
                    tensor_input = torch.tensor([ids], dtype=torch.long)
                    with torch.no_grad():
                        prob = torch.nn.functional.softmax(model(tensor_input), dim=1)
                        pred_class = torch.argmax(prob).item()
                        conf = prob[0][pred_class].item()
                    res_label = "å¥½è¯„" if pred_class == 1 else "å·®è¯„"
                else:
                    res_label = "å¥½è¯„" if "å¥½" in user_input else "å·®è¯„"
                    conf = 0.95
                
            st.markdown("### è¯Šæ–­æŠ¥å‘Š")
            col_res1, col_res2, col_res3 = st.columns(3)
            with col_res1:
                if res_label == "å¥½è¯„": st.success(f"**åˆ¤å®šææ€§ï¼šæ­£å‘**")
                else: st.error(f"**åˆ¤å®šææ€§ï¼šè´Ÿå‘**")
            with col_res2: st.info(f"**æ¨¡å‹ç½®ä¿¡åº¦ï¼š{conf:.2%}**")
            with col_res3: st.warning(f"**æ¶‰åŠç»´åº¦ï¼š{', '.join(aspects)}**")

    elif app_mode == "ğŸ“‚ æ‰¹é‡æŒ–æ˜ (Batch Mining)":
        st.title("ğŸ“‚ æ•°æ®æ‰¹é‡å¯¼å…¥ä¸æŒ–æ˜")
        data_source = st.radio("é€‰æ‹©æ•°æ®æºæ–¹å¼:", ["ğŸ“‚ æœ¬åœ°ä¸Šä¼ æ–‡ä»¶", "ğŸ åŠ è½½ç³»ç»Ÿæ¼”ç¤ºæ•°æ®é›†"], horizontal=True)
        
        df = None
        if data_source == "ğŸ“‚ æœ¬åœ°ä¸Šä¼ æ–‡ä»¶":
            uploaded_file = st.file_uploader("è¯·é€‰æ‹© CSV æ ¼å¼æ•°æ®æ–‡ä»¶", type=["csv"])
            if uploaded_file:
                try: df = pd.read_csv(uploaded_file)
                except UnicodeDecodeError:
                    uploaded_file.seek(0)
                    df = pd.read_csv(uploaded_file, encoding='gbk')
        else:
            if st.button("ä¸€é”®åŠ è½½æµ‹è¯•æ ·æœ¬"):
                if default_data_path and os.path.exists(default_data_path):
                    st.session_state['temp_demo_df'] = pd.read_csv(default_data_path).sample(500)
                else:
                    st.error("æœªæ‰¾åˆ°æ¼”ç¤ºæ•°æ®é›†ã€‚")
            if 'temp_demo_df' in st.session_state and data_source == "ğŸ åŠ è½½ç³»ç»Ÿæ¼”ç¤ºæ•°æ®é›†":
                df = st.session_state['temp_demo_df']
                
        if df is not None:
            cols = df.columns.tolist()
            keywords = ['review', 'è¯„è®º', 'content', 'text', 'å†…å®¹']
            text_col = cols[0] 
            for col in cols:
                if any(k in col.lower() for k in keywords):
                    text_col = col; break
            
            st.info(f"ğŸ’¡ è¯†åˆ«åˆ†æå¯¹è±¡åˆ—ï¼š`[{text_col}]`")
            
            if st.button("ğŸš€ å¯åŠ¨å…¨é‡æ·±åº¦åˆ†æ", type="primary"):
                progress_bar = st.progress(0)
                with st.spinner("æ‰§è¡Œåˆ†æä¸­..."):
                    texts = df[text_col].astype(str).tolist()
                    df['é¢„æµ‹ç»“æœ'] = predict_sentiment(texts, model, vocab)
                    progress_bar.progress(50)
                    df['ç»´åº¦åˆ—è¡¨'] = df[text_col].apply(analyze_aspect)
                    df['æ¶‰åŠç»´åº¦'] = df['ç»´åº¦åˆ—è¡¨'].apply(lambda x: ", ".join(x))
                    progress_bar.progress(100)
                    st.session_state['master_df'] = df
                    st.session_state['text_col'] = text_col
                    st.success("åˆ†æå®Œæˆï¼è¯·å‰å¾€ã€Œç›‘æ§å¤§å±ã€æŸ¥çœ‹ã€‚")
            
        if 'master_df' in st.session_state:
            st.markdown("### æ•°æ®å¯¼å‡º")
            res_df = st.session_state['master_df']
            csv_data = res_df.drop(columns=['ç»´åº¦åˆ—è¡¨'], errors='ignore').to_csv(index=False).encode('utf-8-sig')
            st.download_button("ğŸ“¥ å¯¼å‡ºåˆ†æç»“æœ (CSV)", csv_data, 'output.csv', 'text/csv')

    elif app_mode == "ğŸ“Š ç›‘æ§å¤§å± (Dashboard)":
        st.title("ğŸ“Š å…¨å±€æ•°æ®ç›‘æ§ä¸å¯è§†åŒ–")
        if 'master_df' not in st.session_state:
            st.warning("æš‚æ— æ•°æ®ã€‚è¯·å…ˆæ‰§è¡Œæ‰¹é‡æŒ–æ˜ä»»åŠ¡ã€‚")
        else:
            df = st.session_state['master_df']
            render_dashboard(df)
            
            st.markdown("### â˜ï¸ é«˜é¢‘è¯äº‘æå–")
            c1, c2 = st.columns(2)
            with c1:
                st.markdown("**æ­£å‘ç‰¹å¾**")
                pos_texts = df[df['é¢„æµ‹ç»“æœ'] == 'å¥½è¯„'][st.session_state.get('text_col', 'review')].tolist()
                fig_pos = generate_wordcloud(pos_texts, st.session_state['global_stop_words'])
                if fig_pos: st.pyplot(fig_pos)
            with c2:
                st.markdown("**è´Ÿå‘ç‰¹å¾**")
                neg_texts = df[df['é¢„æµ‹ç»“æœ'] == 'å·®è¯„'][st.session_state.get('text_col', 'review')].tolist()
                fig_neg = generate_wordcloud(neg_texts, st.session_state['global_stop_words'])
                if fig_neg: st.pyplot(fig_neg)

if __name__ == "__main__":
    # ç¡®ä¿æ­¤ä¸ºç¨‹åºçš„é¦–ä¸ª Streamlit æŒ‡ä»¤
    st.set_page_config(page_title="ç”µå•†è¯„è®ºæƒ…æ„Ÿåˆ†æç³»ç»Ÿ", page_icon="ğŸ›ï¸", layout="wide", initial_sidebar_state="expanded")
    
    if 'logged_in' not in st.session_state:
        st.session_state['logged_in'] = False

    if not st.session_state['logged_in']:
        # éšè—å·¦ä¾§è¾¹æ ï¼Œè¥é€ çº¯ç²¹çš„ç™»å½•ç•Œé¢
        st.markdown("<style>[data-testid='collapsedControl'] {display: none;}</style>", unsafe_allow_html=True)
        login_page()
    else:
        main()