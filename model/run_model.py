import pandas as pd
import os
import torch
import torch.nn as nn
import torch.optim as optim
import jieba
from collections import Counter
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
import time

# ==========================================
# ğŸ› ï¸ é…ç½®å‚æ•° (ä½ å¯ä»¥ä¿®æ”¹è¿™é‡Œ)
# ==========================================
MAX_LEN = 50          # å¥å­çš„æœ€å¤§é•¿åº¦
BATCH_SIZE = 64       # æ¯æ¬¡å–‚ç»™æ¨¡å‹å¤šå°‘æ¡æ•°æ®
EMBEDDING_DIM = 100   # æ¯ä¸ªè¯ç”¨å¤šå°‘ç»´çš„å‘é‡è¡¨ç¤º
HIDDEN_DIM = 128      # ç¥ç»ç½‘ç»œéšè—å±‚ç¥ç»å…ƒæ•°é‡
EPOCHS = 10           # è®­ç»ƒå¤šå°‘è½® (å»ºè®® 5-10 è½®)
LEARNING_RATE = 0.001 # å­¦ä¹ ç‡

# ==========================================
# 1. æ•°æ®è¯»å–ä¸å¤„ç† (ä¿æŒä¸å˜)
# ==========================================
def load_and_process_data():
    # --- å®šä½æ–‡ä»¶ ---
    current_dir = os.path.dirname(os.path.abspath(__file__))
    data_path = os.path.join(current_dir, '..', 'data', 'ChnSentiCorp_htl_all.csv')
    
    print(f"ğŸ“‚ æ­£åœ¨è¯»å–æ•°æ®ï¼š{data_path}")
    if not os.path.exists(data_path):
        print("âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶ï¼")
        return None, None, None

    # --- è¯»å–æ¸…æ´— ---
    df = pd.read_csv(data_path).dropna(subset=['review'])
    texts = df['review'].astype(str).tolist()
    labels = df['label'].tolist()
    print(f"âœ… è¯»å–æˆåŠŸï¼å…± {len(texts)} æ¡æ•°æ®")

    # --- æ„å»ºè¯å…¸ ---
    print("ğŸ”¨ æ­£åœ¨æ„å»ºè¯å…¸ (åªä¿ç•™æœ€å¸¸è§çš„5000è¯)...")
    all_words = []
    for text in texts:
        all_words.extend(jieba.lcut(text))
    
    vocab = {"<PAD>": 0, "<UNK>": 1}
    for word, _ in Counter(all_words).most_common(5000):
        vocab[word] = len(vocab)
    
    # --- æ•°å­—åŒ– ---
    print("ğŸ”¢ æ­£åœ¨å°†æ–‡æœ¬è½¬ä¸ºæ•°å­—...")
    input_ids = []
    for text in texts:
        words = jieba.lcut(text)
        ids = [vocab.get(w, 1) for w in words]
        # å¡«å……æˆ–æˆªæ–­
        if len(ids) > MAX_LEN:
            ids = ids[:MAX_LEN]
        else:
            ids = ids + [0] * (MAX_LEN - len(ids))
        input_ids.append(ids)
    
    # --- è½¬ä¸º Tensor ---
    X = torch.tensor(input_ids, dtype=torch.long)
    y = torch.tensor(labels, dtype=torch.long)
    
    return X, y, len(vocab)

# ==========================================
# ğŸ§  2. å®šä¹‰ç¥ç»ç½‘ç»œæ¨¡å‹ (LSTM)
# ==========================================
class SentimentLSTM(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim=2):
        super(SentimentLSTM, self).__init__()
        # 1. åµŒå…¥å±‚ï¼šæŠŠæ•°å­—å˜æˆå‘é‡
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        # 2. LSTMå±‚ï¼šæå–è¯­ä¹‰ç‰¹å¾
        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)
        # 3. å…¨è¿æ¥å±‚ï¼šåˆ†ç±» (å¥½è¯„/å·®è¯„)
        self.fc = nn.Linear(hidden_dim, output_dim)
        
    def forward(self, text):
        # textå½¢çŠ¶: [batch_size, max_len]
        embedded = self.embedding(text) 
        # embeddedå½¢çŠ¶: [batch_size, max_len, embed_dim]
        
        # LSTM è¾“å‡º
        output, (hidden, cell) = self.lstm(embedded)
        # æˆ‘ä»¬åªå–æœ€åä¸€æ­¥çš„éšè—çŠ¶æ€ä½œä¸ºå¥å­çš„ä»£è¡¨
        final_hidden = hidden[-1] 
        
        # åˆ†ç±»
        return self.fc(final_hidden)

# ==========================================
#  3. è®­ç»ƒä¸è¯„ä¼°å‡½æ•°
# ==========================================
def train_model():
    # 1. å‡†å¤‡æ•°æ®
    X, y, vocab_size = load_and_process_data()
    if X is None: return

    # æ‹†åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›† (80% è®­ç»ƒ, 20% æµ‹è¯•)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # åŒ…è£…æˆ DataLoader (æ–¹ä¾¿æ‰¹é‡è®­ç»ƒ)
    train_data = TensorDataset(X_train, y_train)
    test_data = TensorDataset(X_test, y_test)
    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)
    test_loader = DataLoader(test_data, batch_size=BATCH_SIZE)
    
    # 2. åˆå§‹åŒ–æ¨¡å‹
    print(f"\nğŸ§  åˆå§‹åŒ–æ¨¡å‹ (è¯è¡¨å¤§å°: {vocab_size})...")
    model = SentimentLSTM(vocab_size, EMBEDDING_DIM, HIDDEN_DIM)
    
    # 3. å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss() # åˆ†ç±»ä»»åŠ¡æ ‡å‡†æŸå¤±
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    
    # 4. å¼€å§‹è®­ç»ƒå¾ªç¯
    print("ğŸš€ å¼€å§‹è®­ç»ƒ... (è¯·è€å¿ƒç­‰å¾…ï¼Œæ¯è½®å¤§æ¦‚å‡ ç§’é’Ÿ)")
    print("-" * 50)
    
    for epoch in range(EPOCHS):
        start_time = time.time()
        model.train() # å¼€å¯è®­ç»ƒæ¨¡å¼
        total_loss = 0
        correct = 0
        total = 0
        
        for texts, labels in train_loader:
            optimizer.zero_grad()           # æ¸…ç©ºæ¢¯åº¦
            predictions = model(texts)      # å‰å‘ä¼ æ’­ (é¢„æµ‹)
            loss = criterion(predictions, labels) # è®¡ç®—è¯¯å·®
            loss.backward()                 # åå‘ä¼ æ’­ (æ±‚å¯¼)
            optimizer.step()                # æ›´æ–°å‚æ•°
            
            total_loss += loss.item()
            # è®¡ç®—å‡†ç¡®ç‡
            _, predicted = torch.max(predictions, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
            
        train_acc = 100 * correct / total
        
        # --- æ¯è½®ç»“æŸåæµ‹è¯•ä¸€ä¸‹ ---
        model.eval() # å¼€å¯è¯„ä¼°æ¨¡å¼
        test_correct = 0
        test_total = 0
        with torch.no_grad():
            for texts, labels in test_loader:
                outputs = model(texts)
                _, predicted = torch.max(outputs, 1)
                test_total += labels.size(0)
                test_correct += (predicted == labels).sum().item()
        
        test_acc = 100 * test_correct / test_total
        
        print(f"Epoch [{epoch+1}/{EPOCHS}] | "
              f"è€—æ—¶: {time.time()-start_time:.1f}s | "
              f"Loss: {total_loss/len(train_loader):.4f} | "
              f"è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.2f}% | "
              f"æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.2f}%")

    print("-" * 50)
    print("ğŸ‰ è®­ç»ƒç»“æŸï¼æ¨¡å‹å·²ç»å­¦ä¼šäº†åŒºåˆ†å¥½è¯„å’Œå·®è¯„ï¼")
    
    # ä¿å­˜æ¨¡å‹ (æ¯•è®¾éœ€è¦)
    torch.save(model.state_dict(), 'sentiment_model.pth')
    print("ğŸ’¾ æ¨¡å‹å‚æ•°å·²ä¿å­˜ä¸º sentiment_model.pth")

if __name__ == "__main__":
    train_model()