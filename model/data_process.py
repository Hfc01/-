import sys
import os

# ==========================================
# ğŸ‘‡ æ ¸å¿ƒä¿®å¤ä»£ç ï¼šå¼ºè¡ŒæŠŠå½“å‰ç›®å½•åŠ å…¥ç³»ç»Ÿè·¯å¾„
# ==========================================
# 1. è·å– data_process.py æ‰€åœ¨çš„æ–‡ä»¶å¤¹è·¯å¾„ (å³ model æ–‡ä»¶å¤¹)
current_dir = os.path.dirname(os.path.abspath(__file__))
# 2. æŠŠè¿™ä¸ªè·¯å¾„å‘Šè¯‰ Python
sys.path.append(current_dir)

import torch
import jieba
from collections import Counter
# å¼•ç”¨ä½ åˆšæ‰å†™çš„ dataset_loader
from dataset_loader import load_data

# === åŸºç¡€é…ç½® ===
MAX_LEN = 50       # è§„å®šæ¯å¥è¯æœ€é•¿å¤„ç†å¤šå°‘ä¸ªè¯ï¼ˆé…’åº—è¯„è®ºé€šå¸¸ä¸é•¿ï¼Œ50å¤Ÿäº†ï¼‰
BATCH_SIZE = 64    # ä¸€æ¬¡è®­ç»ƒå¤šå°‘æ¡æ•°æ®

# === 1. æ„å»ºè¯å…¸ (Vocabulary) ===
class Vocab:
    def __init__(self, texts):
        self.word2idx = {"<PAD>": 0, "<UNK>": 1} # 0æ˜¯å¡«å……ä½ï¼Œ1æ˜¯æœªçŸ¥è¯
        self.idx2word = {0: "<PAD>", 1: "<UNK>"}
        
        print("æ­£åœ¨æ„å»ºè¯å…¸ï¼Œè¯·ç¨å€™...")
        all_words = []
        for text in texts:
            # ä½¿ç”¨ç»“å·´åˆ†è¯
            words = jieba.lcut(text)
            all_words.extend(words)
        
        # ç»Ÿè®¡è¯é¢‘ï¼Œåªä¿ç•™å‡ºç°æ¬¡æ•°æœ€å¤šçš„å‰ 10000 ä¸ªè¯ (å‡å°‘å™ªéŸ³)
        counter = Counter(all_words)
        common_words = counter.most_common(10000)
        
        for word, _ in common_words:
            if word not in self.word2idx:
                idx = len(self.word2idx)
                self.word2idx[word] = idx
                self.idx2word[idx] = word
        
        print(f"âœ… è¯å…¸æ„å»ºå®Œæˆï¼è¯è¡¨å¤§å°: {len(self.word2idx)}")

    def text_to_ids(self, text):
        # æŠŠä¸€å¥è¯å˜æˆæ•°å­—åˆ—è¡¨
        words = jieba.lcut(text)
        ids = [self.word2idx.get(w, 1) for w in words] # æ‰¾ä¸åˆ°çš„è¯å°±ç”¨1(<UNK>)ä»£æ›¿
        
        # ç»Ÿä¸€é•¿åº¦å¤„ç† (Padding / Truncating)
        if len(ids) > MAX_LEN:
            ids = ids[:MAX_LEN] # æˆªæ–­
        else:
            ids = ids + [0] * (MAX_LEN - len(ids)) # è¡¥0
            
        return ids

# === 2. æ ¸å¿ƒå¤„ç†å‡½æ•° (ç»™åç»­è®­ç»ƒè°ƒç”¨) ===
def get_processed_data():
    # 1. åŠ è½½åŸå§‹æ•°æ®
    texts, labels = load_data()
    
    if texts is None:
        return None, None, None

    # 2. æ„å»ºè¯å…¸
    vocab = Vocab(texts)
    
    # 3. æŠŠæ‰€æœ‰æ–‡æœ¬è½¬æˆæ•°å­—
    print("æ­£åœ¨æŠŠæ–‡æœ¬è½¬åŒ–ä¸ºæ•°å­—åºåˆ—...")
    input_ids = []
    for text in texts:
        ids = vocab.text_to_ids(text)
        input_ids.append(ids)
    
    # 4. è½¬æˆ PyTorch éœ€è¦çš„ Tensor æ ¼å¼
    X = torch.tensor(input_ids, dtype=torch.long)
    y = torch.tensor(labels, dtype=torch.long)
    
    print(f"âœ… æ•°æ®å¤„ç†å®Œæ¯•ï¼è¾“å…¥å½¢çŠ¶: {X.shape}, æ ‡ç­¾å½¢çŠ¶: {y.shape}")
    return X, y, vocab

if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    get_processed_data()